import streamlit as st
import networkx as nx
import base64
import streamlit.components.v1 as components

from llm_engine import generate_learning_path
from graph_builder import build_and_validate_graph
from video_search import fetch_candidate_videos
from reranker import rerank_videos

@st.cache_data(show_spinner=False, ttl=86400) # 24 å°æ™‚å¿«å–ï¼Œé¿å…é »ç¹å‘¼å« LLM API
def get_cached_learning_path(topic: str, language: str):
    """
    åŒ…è£ LLM ç”Ÿæˆé‚è¼¯ã€‚ç›¸åŒçš„ topic èˆ‡ language åœ¨ 24 å°æ™‚å…§åªæœƒæ¶ˆè€—ä¸€æ¬¡ API é¡åº¦
    """
    return generate_learning_path(topic, language)

@st.cache_data(show_spinner=False, ttl=86400) # 24 å°æ™‚å¿«å–ï¼Œé¿å…é »ç¹å‘¼å« Graph å»ºæ§‹é‚è¼¯
def get_cached_videos(node_query: str, topic_name: str, language: str):
    """
    åŒ…è£ youtube æª¢ç´¢èˆ‡ OpenAI é‡æ’åºé‚è¼¯ã€‚ç›¸åŒçš„ç¯€é»æŸ¥è©¢å°‡ç›´æ¥
    """
    candidates = fetch_candidate_videos(topic_name, language)
    if candidates:
        return rerank_videos(node_query, candidates)
    return []

# æ ¹æ“š Graph çš„æ‹“æ¨¸çµæ§‹ï¼Œè¨ˆç®— Mermaid åœ–è¡¨éœ€è¦çš„å‹•æ…‹é«˜åº¦
def calculate_graph_height(G: nx.DiGraph) -> int:
    if not G.nodes:
        return 300
    
    levels = {}
    for n, d in G.nodes(data=True):
        lvl = d.get('level', 0)
        levels[lvl] = levels.get(lvl, 0) + 1
    
    # æ‰¾å‡ºç¯€é»æ•¸é‡æœ€å¤šçš„é‚£å±¤
    max_nodes_in_single_level = max(levels.values())

    # å‡è¨­æ¯å€‹ç¯€é»åœ¨å‚ç›´æ–¹å‘ç´„å  80 åƒç´ ï¼Œä¸¦åŠ ä¸Š 150 åƒç´ çš„åŸºç¤ä¸Šä¸‹é‚Šè·
    calculated_height = (max_nodes_in_single_level * 80) + 150
    
    # æœ€å°‘ 300 åƒç´ ï¼Œæœ€é«˜ä¸è¶…é 1000 åƒç´ 
    return max(300, min(calculated_height, 1000))    

# å°‡ NetworkX åœ–å½¢ç‰©ä»¶è½‰è­¯ç‚º Mermaid çš„æµç¨‹åœ–èˆ‡æ³•
# æ¡ç”¨ Top-Down çš„æ’ç‰ˆ
def convert_nx_to_mermaid(G: nx.DiGraph) -> str:
    mermaid_code = "graph LR\n"

    # å®£å‘Šæ‰€æœ‰ç¯€é»åŠå…¶é¡¯ç¤ºæ–‡å­—
    for node_id, node_data in G.nodes(data=True):
        # è™•ç†åç¨±å¯èƒ½å½±éŸ¿ Mermaid èªæ³•çš„å¼•è™Ÿ
        safe_name = node_data.get('topic_name', node_id).replace('"', "'")
        # ç¯€é» ID ["é¡¯ç¤ºåç¨±"]
        mermaid_code += f'    {node_id}["{safe_name}"]\n'

    # å®£å‘Šæ‰€æœ‰ä¾è³´é€£ç·š
    for source, target in G.edges():
        mermaid_code += f"    {source} --> {target}\n"

    
    return mermaid_code

# Streamlit ç¶²é ä»‹é¢å»ºæ§‹

st.set_page_config(page_title="Learning Path Generator", layout="wide")

# ä½¿ç”¨ session_state ç¢ºä¿é‡æ•´ç•«é¢æ™‚è³‡æ–™ä¸æœƒéºå¤±
if "graph_data" not in st.session_state:
    st.session_state.graph_data = None
if "mermaid_syntax" not in st.session_state:
    st.session_state.mermaid_syntax = None
if "language" not in st.session_state:
    st.session_state.language = "English"

# èªè¨€é¸æ“‡å™¨æ”¾åœ¨æœ€ä¸Šæ–¹
language_choice = st.radio(
    "Language / èªè¨€",
    options=["English", "ç¹é«”ä¸­æ–‡"],
    horizontal=True,
    key="language_selector"
)

st.session_state.language = language_choice

# æ ¹æ“šèªè¨€è¨­å®š UI æ–‡å­—
if language_choice == "ç¹é«”ä¸­æ–‡":
    ui_text = {
        "title": "å­¸ç¿’è·¯å¾‘ç”Ÿæˆå™¨",
        "input_label": "è¼¸å…¥ä½ æƒ³å­¸ç¿’çš„é ˜åŸŸ",
        "input_placeholder": "ä¾‹å¦‚ï¼šè³‡æ–™ç§‘å­¸ã€ç¶²é é–‹ç™¼ã€æ©Ÿå™¨å­¸ç¿’",
        "button": "ç”Ÿæˆå­¸ç¿’è·¯å¾‘",
        "warning": "è«‹è¼¸å…¥æœ‰æ•ˆçš„ä¸»é¡Œ",
        "spinner": f"æ­£åœ¨åˆ†æä¸¦ç”Ÿæˆå­¸ç¿’è·¯å¾‘...",
        "error": "ç”Ÿæˆéç¨‹ç™¼ç”ŸéŒ¯èª¤",
        "graph_title": "ç”Ÿæˆçš„å­¸ç¿’è·¯å¾‘",
        "details_title": "ç¯€é»è©³ç´°è³‡è¨Š",
        "level": "å±¤ç´š",
        "description": "æè¿°",
        "estimated_hours": "é ä¼°æ™‚æ•¸",
        "hours": "å°æ™‚",
        "key_concepts": "é—œéµæ¦‚å¿µ",
        "actionable_steps": "å­¸ç¿’æ­¥é©Ÿ",
        "no_description": "ç„¡æè¿°",
        "find_videos": "å°‹æ‰¾ç²¾é¸æ•™å­¸å½±ç‰‡ ğŸ¬",
        "video_spinner": "æ­£åœ¨æª¢ç´¢ä¸¦ä½¿ç”¨ AI é€²è¡Œèªæ„é‡æ’åº (Reranking)...",
        "video_top3_title": "ğŸ† AI ç²¾é¸ Top 3 æ•™å­¸è³‡æº",
        "video_rank_label": "ç¬¬ {rank} åï¼šèªæ„å¥‘åˆåº¦ `{pct}%`",
        "video_not_found": "æ‰¾ä¸åˆ°é©åˆçš„å½±ç‰‡ï¼Œæˆ– YouTube API ç™¼ç”ŸéŒ¯èª¤ã€‚"
    }
else:
    ui_text = {
        "title": "Learning Path Generator",
        "input_label": "Enter a Learning Topic",
        "input_placeholder": "e.g., Data Science, Web Development, Machine Learning",
        "button": "Generate Learning Path",
        "warning": "Please enter a valid learning topic.",
        "spinner": f"Analyzing and generating learning path...",
        "error": "An error occurred while generating the learning path",
        "graph_title": "Generated Learning Path",
        "details_title": "Detailed Learning Nodes",
        "level": "Level",
        "description": "Description",
        "estimated_hours": "Estimated Hours",
        "hours": "hours",
        "key_concepts": "Key Concepts",
        "actionable_steps": "Actionable Steps",
        "no_description": "No description available",
        "find_videos": "Find Curated Tutorial Videos ğŸ¬",
        "video_spinner": "Fetching candidates and reranking with AI semantics...",
        "video_top3_title": "ğŸ† AI Top 3 Recommended Learning Videos",
        "video_rank_label": "Rank #{rank}: Semantic Match `{pct}%`",
        "video_not_found": "No suitable videos found, or YouTube API request failed."
    }

st.title(ui_text["title"])

topic_input = st.text_input(ui_text["input_label"], placeholder=ui_text["input_placeholder"])

if st.button(ui_text["button"]):
    if not topic_input.strip():
        st.warning(ui_text["warning"])
    else:
        with st.spinner(ui_text["spinner"]):
            try:
                # å°‡é¸æ“‡çš„èªè¨€ä½œç‚ºåƒæ•¸å‚³éçµ¦å¾Œç«¯ API
                path_data = get_cached_learning_path(topic_input, language_choice)

                # å»ºæ§‹ NetworkX åœ–å½¢ä¸¦é©—è­‰é‚è¼¯
                G, node_levels = build_and_validate_graph(path_data)

                # è½‰è­¯ç‚º Mermaid èªæ³•
                mermaid_syntax = convert_nx_to_mermaid(G)

                # å°‡çµæœå­˜å…¥ç‹€æ…‹ä¸­
                st.session_state.graph_data = G
                st.session_state.mermaid_syntax = mermaid_syntax
            except Exception as e:
                st.error(f"{ui_text['error']}: {e}")

# æ¸²æŸ“çµæœ
if st.session_state.mermaid_syntax and st.session_state.graph_data:
    st.markdown(f"### {ui_text['graph_title']}")
    
    mermaid_code = st.session_state.mermaid_syntax
    
    # 1. å°‡èªæ³•ç·¨ç¢¼ç‚º Base64ï¼Œé¿å…å¼•è™Ÿèˆ‡æ›è¡Œç¬¦è™Ÿç ´å£ HTML çµæ§‹
    b64_code = base64.b64encode(mermaid_code.encode("utf-8")).decode("utf-8")
    
    # 2. å»ºæ§‹å®¢è£½åŒ–äº’å‹•è¦–çª—çš„ HTML/JS è…³æœ¬ (ä¿®æ­£å¥—ä»¶è¼‰å…¥éŒ¯èª¤èˆ‡æ–°å¢éŒ¯èª¤æ•æ‰)
    html_code = f"""
    <!DOCTYPE html>
    <html>
    <head>
        <script src="https://cdn.jsdelivr.net/npm/svg-pan-zoom@3.6.1/dist/svg-pan-zoom.min.js"></script>
        <style>
            body {{ margin: 0; padding: 0; background-color: transparent; }}
            #container {{ width: 100vw; height: 100vh; overflow: hidden; }}
        </style>
    </head>
    <body>
        <div id="container"></div>
        <script type="module">
            import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
            
            mermaid.initialize({{ startOnLoad: false, theme: 'default' }});
            
            try {{
                // è§£ç¢¼ Base64 å–å¾— Mermaid èªæ³•
                const code = decodeURIComponent(escape(window.atob('{b64_code}')));
                
                mermaid.render('graphSvg', code).then((result) => {{
                    document.getElementById('container').innerHTML = result.svg;
                    const svgElement = document.querySelector('#container svg');
                    
                    // å¼·åˆ¶è®“ SVG å¡«æ»¿æˆ‘å€‘è¨­å®šçš„è¦–çª—å¤§å°ä¸¦è§£é™¤é è¨­å¯¬åº¦é™åˆ¶
                    svgElement.style.width = '100%';
                    svgElement.style.height = '100%';
                    svgElement.style.maxWidth = 'none';
                    
                    // å‘¼å«å…¨åŸŸçš„ svgPanZoom å‡½æ•¸ä¾†ç¶å®šæ”¾å¤§ç¸®å°åŠŸèƒ½
                    window.svgPanZoom(svgElement, {{
                        zoomEnabled: true,
                        controlIconsEnabled: true,
                        fit: true,
                        center: true,
                        minZoom: 0.2,
                        maxZoom: 10
                    }});
                }}).catch(err => {{
                    document.getElementById('container').innerHTML = '<p style="color:red; padding:20px;">åœ–è¡¨æ¸²æŸ“å¤±æ•—: ' + err.message + '</p>';
                }});
            }} catch (err) {{
                document.getElementById('container').innerHTML = '<p style="color:red; padding:20px;">è…³æœ¬åŸ·è¡ŒéŒ¯èª¤: ' + err.message + '</p>';
            }}
        </script>
    </body>
    </html>
    """
    
    # 3. æ¸²æŸ“é«˜åº¦å›ºå®šç‚º 600px çš„äº’å‹•å¼è¦–çª—
    components.html(html_code, height=600, scrolling=False)

    # ä¾æ“š level é€²è¡Œåˆ†çµ„èˆ‡æ’åºé¡¯ç¤º
    G = st.session_state.graph_data
    levels = {}
    for n, d in G.nodes(data=True):
        lvl = d.get('level', 0)
        if lvl not in levels:
            levels[lvl] = []
        levels[lvl].append((n, d))

    st.markdown(f"### {ui_text['details_title']}")
    # --- ä¸‹æ–¹ç¶­æŒåŸæœ¬çš„ for lvl in sorted(levels.keys()): è¿´åœˆé‚è¼¯ ---
    # é€éå±¤ç´šæ’åºï¼Œä¾åºåˆ—å‡ºæ¯å€‹ç¯€é»çš„å…·é«”å­¸ç¿’å…§å®¹

    for lvl in sorted(levels.keys()):
        st.subheader(f"{ui_text['level']} {lvl}")
        for node_id, data in levels[lvl]:
            with st.expander(f"{data.get('topic_name', node_id)}"):
                # é¡¯ç¤ºç¯€é»çš„æè¿°ã€æ ¸å¿ƒæ¦‚å¿µã€é ä¼°å­¸ç¿’æ™‚é–“ç­‰è³‡è¨Š
                st.markdown(f"**{ui_text['description']}:** {data.get('description', ui_text['no_description'])}")
                est_hours = data.get('estimated_hours', 0)
                st.markdown(f"**{ui_text['estimated_hours']}:** {est_hours} {ui_text['hours']}")

                concepts = data.get('key_concepts', [])
                if concepts:
                    st.markdown(f"**{ui_text['key_concepts']}:** {', '.join(concepts)}")

                steps = data.get('actionable_steps', [])
                if steps:
                    st.markdown(f"**{ui_text['actionable_steps']}:**")
                    for step in steps:
                        st.markdown(f"- {step}")

                st.divider()
                if st.button(ui_text["find_videos"], key=f"btn_vid_{node_id}"):
                    with st.spinner(ui_text["video_spinner"]):
                        # çµåˆä¸»é¡Œèˆ‡è©³ç´°æè¿°ï¼Œè®“ Embedding å…·å‚™æ›´è±å¯Œçš„ç‰¹å¾µ
                        node_query = f"{data.get('topic_name', '')} {data.get('description', '')}"
                        # å–å¾—å‰ 10 åå€™é¸å½±ç‰‡ Metadata
                        ranked_videos = get_cached_videos(node_query, data.get('topic_name', ''), language_choice)

                        if ranked_videos:
                            top_3_videos = ranked_videos[:3]
                            st.markdown("### AI-selected Top 3 teaching resources")

                            st.markdown(f"#### {ui_text['video_top3_title']}")
                            # ä½¿ç”¨è¿´åœˆå‚ç›´æ¸²æŸ“å‰ä¸‰åå½±ç‰‡èˆ‡å…¶åˆ†æ•¸
                            for index, vid in enumerate(top_3_videos, start=1):
                                score = vid.get('similarity_score', 0)
                                # å°‡åˆ†æ•¸è½‰æ›ç‚ºæ˜“è®€çš„ç™¾åˆ†æ¯”æ ¼å¼
                                match_percentage = round(score * 100, 1)
                                # é¡¯ç¤ºæ’åèˆ‡å¥‘åˆåº¦
                                st.markdown(
                                    ui_text["video_rank_label"].format(
                                        rank=index,
                                        pct=match_percentage,
                                    )
                                )
                                st.video(f"https://www.youtube.com/watch?v={vid['id']}")
                        else:
                            st.warning(ui_text["video_not_found"])